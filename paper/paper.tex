% !TEX program = xelatex

\documentclass[paper=a4,fontsize=12pt,twocolumn]{scrreprt}

\usepackage{fontspec}
\usepackage{polyglossia}
\setmainlanguage[babelshorthands=true]{german}

\usepackage[autostyle,german=quotes]{csquotes}
\usepackage[autostyle]{csquotes}

\usepackage[toc,page]{appendix}

\usepackage{array}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}

\usepackage{graphicx}

\usepackage{kantlipsum}

\usepackage[
    backend=biber,
    style=numeric,
    citestyle=authoryear,
]{biblatex}

\addbibresource{./literature.bib}
\graphicspath{ {./images/} }

\usepackage{amsmath}

\title{Project::Foo Abschlussbericht}
\author{Maximilian Bauregger \and Leonard Caanitz \and Lennart Clasmeier \and Patrizio Ferrara \and Luca Müller \and Frederic Voigt}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section*{Abstract}

\begin{itemize}
    \item Was wurde wie erreicht?
    \item Wo pflegt sich das in den den aktuellen stand der Forschung ein - wir haben also eine reduzierte ARM Version gebaut
\end{itemize}
\kant[1]


\chapter{Einführung}
\begin{itemize}
    \item Hier wird das Projekt kurz vorgestellt
    \item Was haben wir gemacht? Was haben wir vor? Welche Tools wurden benutzt?
    \item Welche Schwierigkeiten haben sich abgezeichnet?
    \item welche Struktur hat der Vortrag - Welche Informationen können die Leser\_Innen in welchen Kapiteln erwarten.
\end{itemize}
\kant[2-3]

\chapter{Planungsphase}

%Um die vorgegebenen Ziele zu erfüllen, sind wir systematisch an das Projekt herangegangen. %% TODO kann weg
Zuerst haben wir in einem Python Sketch die Struktur des Prozessors definiert.
Danach wird gemeinsam über den möglichen Befehlssatz diskutiert.
Nachdem diese Dinge geklärt sind, haben sich die Gruppen in den jeweiligen Bereich Software und Hardware aufgeteilt, um so das Besprochene zu realisieren.
Im Anschluss soll die programmierte Hardware und Software zusammen verbunden, getestet und eventuelle Fehler behoben werden, bis keine Fehler vorhanden sind.
Falls noch Zeit übrig sein sollte, sind Erweiterungen des Befehlssatzes, sowie weitere Datentypen mögliche Punkte, um den Prozessor qualitativ zu verbessern.
Während der Bearbeitung des Projekts sind einige Fragestellungen und Diskussionspunkte aufgekommen, die im Nachfolgenden genauer besprochen werden sollen. Darunter finden sich auch Punkte oder Konzepte, die im Diskussionsprozess ausgeschlossen und nicht im finalen Projekt manifestiert wurden. Diese wollen wir dennoch beleuchten.

%% TODO Kein Wasserfall

\section{Aufbau des Prozessors}

\begin{itemize}
    \item Python Sketch als Struktur (?)
    \item Pipelining (?)
\end{itemize}



%\section{IS}

%\begin{itemize}
%    \item 32 Bit Befehlswortlänge (x)
%   \item 3 Addressmaschine (x)
%    \item reduzierter ARM Befehlssatz (x)
%    \item Opcode-Länge (x)
%    \item Immediate Bit (x)
%    \item Relative Jumps (x)
%    \item Relative Speicheraddresssierung (x)
%    \item Struktur des Befehlwortes (x)
%    \item immediate verwenden (letzten 16 Bit)(x)
%    \item 2er-Komplement (x)
%    \item Int als einziger Datentyp (x)
%    \item 32 Register (x)
%    \item 0 Register (x)

%\end{itemize}

// appendix full instruction set


\kant[5]


\section{Instructionset}


%%TODO Eingführung ins Kapitel Instruction Set sschreiben, was wird in welcher tiefe vorgetsellt was passiert in welchen kapitel später mit deiesen infromationen, wozu ist das wichtig?

\kant[6]

%118 -128

\subsection{Wortlänge} 

Als erster großer Diskussionspunkt gab es die Entscheidung der Wortlänge.
Zur Auswahl standen 32 oder 64 Bit. Alle kleineren Längen haben wir ausgeschlossen, da wir 16-Bit immidate-Werte haben wollten. Zudem waren uns kleinere Längen als 32 Bit zu kompliziert in der Ausarbeitung gewesen.
Die 32 bzw. 64 Bit Wortlänge bietet den Vorteil, dass große Intermediates genutzt werden können und bei der Wahl des OP-layouts die volle Entscheidungsfreiheit bleibt.
Gegen die 64 Bit-Länge haben wir uns unter dem Gesichtspunkt entschieden, dass in unserem Zugrunde liegenden Instruction-set, die Wortlänge kaum ausgeschöpft werden würde. 
%Todo warum genau 64 bit besser

%132 - 148

\subsection{Adressmaschine}
Auf Basis der Wortlänge haben wir uns nun der Frage der Adressmaschine zugewandt. Wir hatten die Alternativen 3 oder 2 Adress Machine. Eine 1-Adress Maschien wurde als nicht-zielführend ausgeschlossen. Die Implementierung einer Pipeline war von vornherein angedacht. In einer 1-Adress Maschine hätte für viele Operationn (die in einer 3-Adress Maschien einen Befehl brauchen) mehrere Befehle gebraucht. Das hätte zu einer hohen Zahl an Abhängigkeiten innerhalb der Implementation und ständigen flushes geführt. Gleiches Problem ergibt sich bei einer 2-Adress Maschine, wenn auch nicht so stark. 

Darum wurde sich schlussendlich auch für eine 3-Adressmaschine entschieden. Unter der Wortlänge wurde eine 3-Adressmaschine als implementierbar erachtet. Im Bezug auf das angestrebte Pipelining wurde so die Menge an benötigten Befeheln reduziert. Das ist auch für die spätere Software-Implementierung einfacher. 

\subsection{Befehlssatz}
{\color{red}
Nachdem die Länge und die Adressierung des Befehlswortes beschlossen wurde, mussten die einzelnen Befehle festgelegt werden.
Dabei wurde sich an den ARM Befehlssatz orientiert\footnotemark.
\footnotetext{//(*link zum ARM Befehlssatz)//}
Dieser wurde noch reduziert. Bei der Reduzierung wurden die Befehle in 3 Kategorien unterteilt:
\begin{itemize}
    \item \enquote{must-have}:
    Das Minimum das nach erachtens der Gruppe ein Prozessor haben muss.
    \item \enquote{nice-to-have}:
    Funktionen die bei übrig gebliebener Zeit implementiert werden können.
    \item \enquote{irrelevant}:
    Zu komplexe oder für das Projekt irrelevante Funktionen.
\end{itemize}
}
% Wir haben uns das bare minimum da raus gepickt, von dem wir glaubten, das braucht ein prozessor

% die befehle passten in 5 bit opcode
% Aufteilung in one, two, three op ??

Folgend der Auswahl des Befehlssatzes, mussten die einzelnen Befehle einem Opcode zugewiesen werden.
{\color{red} Hierbei musste entschieden werden, ob bei direkten Eingaben mit einem Immediate-Wert ein neuer Befehlscode hinzugefügt werden soll. Eine andere Möglichkeit ist  die Unterscheidung nicht im Opcode stattfinden zu lassen, sondern in einem weiteren Bit auszulagern. Beide Ansätze führen beim 32-Bit-Code schlussendlich zum selben Ergebnis. 
Vorteile einer Auslagerung ist die geringere Anzahl an Befehle die benötigt wird. Dadurch hat die Softwareprogramierung eine Vereinfachung, da sie zwischen einer direkten und indirekten Zuweisung nicht unterscheiden muss. Da diese Unterscheidung nicht im Opcode behandelt wird, wird ein Bit in der 32-Bit-Anweisung als \enquote{Immediate-Bit} deklariert.} 

%Bei einer Auswahl von 18 Befehlen und zusätzlich bei einigen Befehlen die Unterscheidung zwischen einer direkten Eingabe über einen Immediate-Wert oder einer indirekten Angabe über eine Adresse kam die Überlegung, ob dem Opcode eine Länge von 5-Bit ausreichend ist oder auf 6-Bit erhöht werden muss.
% Die Debatte war doch, ob wir ein dediziertes immediate bit und 5bit opcodes haben oder "immediate befehle" und dann 6bit opcodes.

%Da die Hardware den selben Arbeitsaufwand bei einem Opcode der Länge von 5- oder 6-Bit hat, haben wir uns entschieden die Softwareprogrammierung zu vereinfachen und einen Opcode zu erstellen, der zwischen einer Direktzuweisung und einer indirekten Zuweisung nicht unterscheidet.
%Sodass auch bei 18 Befehlen ein 5-Bit langer Opcode ausreichend ist und noch genügend nicht zugewiesene Codes hat, um den Befehlssatz noch zu erweitern.




% Wir haben uns entschieden, den opcode 5bit lang zu lassen und ein dediziertes immediate bit zu setzen:
% das ist konsistent und macht es einfach zu entscheiden, ob immediate oder register gelesen wird.
%Das hat den Hintergrund, dass wir die dadurch entstehende Struktur als sinnvolle Ordnung der OP Codes ansehen. kann weg? 
So waren wir in der Lage in unserer finalen Implementation, jeden der Befehle durch ein einziges Befehlswort auszudrücken und brauchten keine weitere Implementation für die immediate Version.
Beispielsweise gibt es nur den Befehl \texttt{ADD}, statt \texttt{ADDI} für Immediate.
Dieses Bit soll \enquote{0} sein, falls der Befehl eine Registerzuweisung ist, und \enquote{1}, falls es eine Direktzuweisung gibt.
Somit wird im Befehlswort ersichtlich, was zugewiesen wird.

{\color{red}Durch diese Entscheidungen wurden 18 Befehle in 5-Bit langen Opcodes zugewiesen. Die 18 Befehlte wurden in 3 Kategorien unterteilt. Diese unterscheiden sich in der Anzahl der Parameter die angegeben werden müssen. Um diese Unterscheidung auch im Opcode erkenntlich zu zeigen, haben alle 3 Kategorien(ONEOP, TWOOP,THREEOP) unterschiedliche erste 2 Bits in ihrem Opcode. Sodass eine konsistente Regel ihre Layout hergestellt werden kann.

BILD/TABELLE DER MÖGLICHEN LAYOUTS

Durch diese Unterteilung, kann es später im Dekodierungsprozess zu Zeitersparungen kommen, da in Abhängigkeit der ersten 2 Bits der Decoder weiß in welchen folgenden Bits die mitgeführte Information enthalten ist.

{\color{green} WORK IN PROGRES}

Bei diesen Unterteilungen kam die Frage auf, welche Befehle wie viele Parameter haben sollen.
Betroffen waren vor allem 3 Befehle: der \enquote{Jump}-Befehl und der \enquote{Store}- bzw. \enquote{Load}-Befehl.}
Bei diesen Befehlen ging es vor allem darum, dass ein Betriebssystem den Speicher reserviert und somit nur Anfangs- und Endadresse der Reservierung kennt.
% Das war nicht die debatte hier, sondern wieviele Operanten haben die Befehle. Also ist Jump ein oneop oder threeop.
Ein Threeop Jump Befehl hat den Vorteil, dass mehrere Programme gleichzeitig laufen und man die \enquote{echte} Speicheradresse nicht kennen muss.
% "Das relative Sprünge möglich sind"
%..man weiß umformulieren
Dies ist zB. relevant um Arrays einfach adressieren zu können.
Da wir aber in diesem Projekt ohne Betriebssystem arbeiten, war es fraglich, ob diese Ergänzung zum Befehl einen Nutzen haben würde.
Der ausgeführte Code ist selbst geschrieben und damit ist jede Zuweisung der einzelnen Speicheradressen bekannt.
%Absatz darunter neu 
So entfällt der Vorteil des indirekten Ansprechens der Speicheradressen, da diese immer direkt angesprochen werden können.
Zusätzlich laufen keine weiteren Programme parallel, sodass auch dieser Vorteil hinfällig ist.

Da es aber unser Ziel ist einen möglich guten Prozessor zu erstellen, wurden die direkten Sprünge und Speicheradressierungen mit implementiert, da diese die Qualität des Prozessors steigern und eventuelle weitere Arbeiten mit dem Prozessor vereinfachen.

Abschließend musste noch die Struktur des Befehlswortes festgelegt werden. Dabei unterscheiden wir zwischen Befehlen die einen, zwei oder drei Eingabeparameter haben.
Alle haben gleichartige Strukturelemente.

(Von links nach rechts) Die ersten 5 Bits sind der Opcode, das 6. Bit, ist das Immediate-Bit, und falls ein Immediate-Wert vorhanden ist, befindet er sich immer an den letzten 16 Stellen.

\enquote{Opcode/ \enquote{0}/ Zuweisungen}  // BILD \footnote{BILD}

\enquote{Opcode / \enquote{1} / Zuweisungen / Immediate} // BILD\footnote{BILD}

{\color{green}END WORK IN PROGRES}

{\color{red}
Die 16 Bits wurden ausgewählt, da die Register einen Speicher von 32 Bit haben und somit 16 Bits einfacher zu behandeln sind. Zahlen größer 16 Bits in 3 Schritten dargestellt werden. 
\begin{itemize}
    \item 1. Direktzuweisung der ersten 16 Bits in ein Register
    \item 2. Das zugewiesene Register wird um 16 Stellen nach links geschoben (\enquote{Shift 16})
    \item 3. Direktzuweisung der letzten 16 Bits in das Register
\end{itemize}
    \texttt{(Dies ist ein Beispiel für eine 32Bit große Zahl)}
}

{\color{green} WORK IN PROGRES}

Je nach Eingabeparameter sieht die Struktur der Instruktionen folgendermaßen aus :

// BILD\footnote{BILD}

Aus diesen Überlegungen ergab sich folgende Befehlsstruktru: Die ersten fünf bit stehen für den Opcode.
Das darauf folgende Bit signalisiert ob es sich beim letzten Operanden um ein Emidiate Wert handelt.
Die restlichen Bits sind für die Operanden reserviert.
Instruktionen haben felder für register / immediate werd
Sind 5bit lang (immediate 16)
%ganze Befehlsstruktur einmal schriftlich niederlgen, vor diesem Teil
Durch die Begrenzung der Eingabeparameter auf 5 Bits können somit 32 Register benutzt werden.

{\color{green}END WORK IN PROGRES}

%kann weg oder deutlich kürzer formulieren 
% 2er komplemnt kann in alu erwähnt, hat nix mit 0 register zu tun 
%inetger ist quatsch 
%Aus Einfachheit, für die Berechnungen der Alu, wurde weiterhin beschlossen, dass das "Register-0" ein 0-Register bleibt und alle Berechnungen im 2er-Komplement getätigt werden.
%Weiterhin wurde entschieden den Integer als einzigen Datentyp zu erlauben.
% Alu rechnet einfahc nur, 2er komplement ist nur interpretation

%\subsection{16 bit vs. 32 bit vs. 64 bit (x)}
%Wir haben uns für 32 bit entschieden, warum?
%\begin{itemize}
%    \item genug platz für große immediates und 3 op layout
%    \item nicht "unnötig groß" z.B. beim debuggen einfacher 32 bit zu nutzen als 64
%\end{itemize}

%\subsection{5bit opcode vs 6bit opcode(x)}
%Wir haben 5opcode mit nem zusätzlichen immediate bit, warum?
%\begin{itemize}
%    \item sinnvolle Ordnung der Opcodes
%    \item Nur ein Mnemonic im assembler für add sub etc. (kein addi etc.)
%\end{itemize}

%\subsection{3 Addressmaschine? abhängig von der bit-Zahl(x)}
%32 bit erlauben uns 3 address op codes

%\subsection{Relative Jumps (nur immediate jumps?)(x)}
%Wir haben uns für relative und absolute jumps entschieden (JMP ist ein TWOOP).


\chapter{Hardware}

% TODO Missing: Aufstellung von Problemen

\begin{itemize}
    \item VHDL
    \begin{itemize}
        \item Struktur der Komponenten
        \item Zu "wörtliche" Umsetztung des Python-Sketches (alles in entities verpackt, wir wussten nicht wie Register funktionieren)
        \item Probleme mit dem reinen Entity - Architecture - Ansatz
        \item Parallelität und sequenzielle Abläufe
        \item Clock
        \item Pipeline Nutzung
        \item Finaler Entwurf
    \end{itemize}
    \item Tests
    \item Nice additions
\end{itemize}

\section{Vorwort}

Dieses Kapitel behandelt die Umsetzung unseres Mikroprozessors in Hardware Komponenten in VHDL.
Zeitlich betrachtet schließt dieses Kapitel also an die Arbeitet des Hardware Teams nach der gemeinsamen Planungsphase des gesamten Teams an.
%eventeull als zeitliche einleitung für software übernehmen 
In dieser vorangegangenen Planungsphase einigte sich die Gruppe auf zentrale Orientierungspunkte, welche die Arbeit des gesamten Teams strukturieren.
Diese wurde bereits in Kapitel (?!) vorgestellt.
Dazu zählen fundamentale Konzepte, wie die Wortbreite und der entlehnte ARM-Befehlssatz (siehe Anhang).
% Referenzen schönmachen
Die durch das Team geleistete Vorbereitung stellt uns einen detaillierten Leitfaden zur Umsetzung bereit.
Auf dem Papier ist bereits an diesem Punkt ein Prozessor in Python Pseudocode entstanden, der zur Orientierung dienen sollte (siehe Anhang), allerdings ohne Pipelining.
%pseudo code ist in pipeline allerdings funktional ohne zeitverhalten
Praktisch musste dann nur der Python Code in funktionierenden VHDL Code umgewandelt werden. Das stellte die erste Herausforderung da, denn von uns hatte niemand Erfahrung auf dem Gebiet von Hardware-Beschreibungssprachen.
Intern hat sich das Hardware-Team darauf geeinigt sich mindestens einmal pro Woche zu Treffen um erarbeitete Lösungen zu besprechen, das weitere Vorgehen zu Planen und die Arbeit aufzuteilen.
%so etwas auch für software Einleitung vorgehen dies das 

Im Folgenden soll einerseits der Prozess aus Sicht des Hardware Teams, der schlussendlich zu einer funktionierenden Lösung geführt hat, beschrieben werden.
Andererseits soll die von uns erarbeitete Lösung vorgestellt und diskutiert werden.
% Der folgende Ttext gibt einen einblick in den Prozess und stelltt die Inhaltichen schrittte vor.
Im Fokus stehen dabei einerseits der .

\section{Vorgehen}

Als Hardware-Team haben wir uns intern darauf geeinigt uns einmal pro Woche zu treffen.
%steht genau so in der Einleitung
Hier wurden selbstständig erarbeitete Lösungen, Probleme und die Arbeitsteilung besprochen.
Da für alle beteiligten Programmieren in VHDL neu war, haben wir uns in der frühen Phase des Projekts bzw. unmittelbar am Anfang auf ein Drei-Phasen-Programm geeinigt.
In der ersten Phase haben alle Teammitglieder sich allgemein mit VHDL vertraut gemacht.
Wie blieb den einzelnen Gruppenmitgliedern überlassen.
Vielversprechendes Material wurde im GitHub gesammelt.
Darüber hinaus wurden Schwerpunkte vergeben zu denen unter anderem Entities, Architectures, Prozesse und mögliche Umsetzungen von Pipelining gehörten.
Als Ergebnis dieser ersten Recherchen haben wir uns für einen strengen Entity Architecture Ansatz entschieden.
% was ist ein "strenger entity architecture" ansatz genau?
Es erschien uns zu diesem Zeitpunkt logisch jeder Stage im Prozessor seine eigene Entity und Architecture zu geben.
Die in den Entities definierten Schnittstellen sollten die Kommunikation der einzelnden Stages untereinander gewährleisten, während die jeweilige Architectures die gewünschte Datenverarbeitung übernähme.
In der zweiten Phase wurden die Stages aufgeteilt, sodass jeder mindestens eine Komponente in VHDL programmiert hat.
Dabei stand nicht im Vordergrund eine perfekte Lösung zu erarbeiten.
Viel mehr sollte ein Rahmen konstruiert werden.
In der dritten Phase würden dann die individuell erstellten Komponenten - so unsere Vorstellung - nach Baukastenprinzip zusammengesetzt und so lange verbessert werden bis ein lauffähiger Prototyp entsteht.
Bei diesem Prozess haben wir zunächst noch einmal von unserem Entwurf abstrahiert und in der Umsetzung reduziert.
So war zum Beispiel Pipelining kein Teil dieser Lösung.
%klären inwie weit pipelining doch schon teil der lösung war 
%piplininggg war schon imemr teiol der löszung, richtiger wäre wir hatten keine ahnung und haben pipeline register eher aussgespart bis wir dann wussten wie das ging
Der Fokus lag darauf lauffähigen VHDL-Code zu produzieren.

\begin{figure}
    \centering
    % \includegraphics{}
    \caption{Hier kommt vielleicht nochmal ne Grafik hin}
    \label{fig:my_label}
\end{figure}

% My gang,
% ( ͡°( ͡° ͜ʖ( ͡° ͜ʖ ͡°)ʖ ͡°) ͡°)
%
% my dollars
% [̲̅$̲̅(̲̅ ͡° ͜ʖ ͡°̲̅)̲̅$̲̅]
%
% my Voldemort
% ( ͡° ͜V ͡°)

Dieser strikte Entity-Archittecture-Ansatz führte jedoch nicht zu einer Lösung und so ist die dritte Phase in der Form nie eingetreten.
Der von uns verfolgte Ansatz führte aber insofern zum Erfolg als dass alle Teammitglieder erste theoretische und praktische Grundlagen in VHDL erlangt haben.
Dadurch waren alle Teammitglieder in der Lage sich an Diskussionen zu beteiligen.
Durch die individuelle Spezialierung konnten wir uns gegenseitig Hilfestellung leisten und es fielen Probleme auf, die andern zunächst verborgen blieben.

Zeitlich gesehen stellt sich dieser Punkt als Halbzeit des Projekts dar.
%DAs nahm mehr als die hälfte der verwendetten zeitt ein
Bis dahin haben wir E/A für CPU, ALU, Decoder, Sign-Ext, PC und Pipeline Register geschrieben.
In der Diskussion um Flag Handling wurde uns klar, dass wir grundlegende Konzepte in VHDL nicht oder falsch verstanden haben.
Am prominentesten ist das Konzept von Nebenläufigkeit in VHDl zu nennen und die Frage danach wie genau Prozesse funktionieren.
Der Umstand, dass Prozesse nebenläufig sind, die Statements innerhalb aber sequenziell abgearbeitet werden, führte im Folgenden immer wieder zu Problemen {\color{red}ZU WELCHEN EIGENTILICH??}
In der Retrospektive könnte man sagen: \enquote{Wir haben zu funktional gedacht}.

%Erzählperspektive


% Vielleicht kommt das auch einfach raus oder woanders hin?
%Es klingt vielleicht naiv aber die Erkenntnisse die Signale so zu betrachteten wie elektrische Leitungen in Silikon(Silizium ;) ) war ein gedanklicher Wendepunkt in unserer Arbeit.  {\color{red}Das ist irgendwie noch sehr holprig und belletristisch, da muss man nochmal ran}

%Weil wir einfach die Signale durch pusten von Stage zu ssttaaggee durchpusten wollten und die Nebenläufigkeit der Komponenten total außer acht gelassen haben bzw. uns darüber nicht im klaren waren,

Bis dato existierte noch kein Speicher und Pipelining war auch noch nicht implementiert.
Wir mussten unseren Ansatz noch einmal neu denken. Genauer: neu darüber nachdenken, wie sich unser Ansatz in VHDL sinnvoll umsetzten lässt.
Im folgenden Kapitel sollen die Probleme und die anschließend daraus gezogenen Schlüsse kurz Diskutiert werden um im Anschluss in [Kapitel BLANK] den letztendlichen Entwurf vorzustellen. 

\subsection{Vorläufige Problemsammlung}
% feels like a leap - vllt anderer titel?

%TEILL DAS IN DIE KAPITEL AUF


Hier werden einige Probleme unserer ersten besprochen und Lösungsansätze kurz besprochen werden. Dabei soll davon abgesehen werden jede kleine Änderung im Code abzuarbeiten welche die letztendliche Lösung zum Resultat hatten. Viel mehr soll ein Eindruck davon vermittelt werden welche entscheidenden Impulse diese Auseinandersetzung geliefert um im nächsten Kapitel unsere Lösung detailliert vorzustellen.
Wir mussten Feststellen, dass immer wieder undefinierte Werte in unseren Registern aufgetaucht sind oder Programme falsche Ergebnisse lieferten.
    Dass lag daran, das falsche Werte propagiert wurden und/oder gleichzeitige Zugriffe auf Signale stattfanden.
    Hier rächte sich ein wenig unsere Idee zunächst einen lauffähiges Programm zu produzieren bevor wir uns an die Feinheiten machen wollten.




\begin{itemize}
    \item 
    %Gedanklich sind wir aber schon von Pipelining ausgegangen und haben die Parallelität der Komponenten außer Acht gelassen.
    Signale auf denen Werten an lagen wurden weiter propagiert und von den nachkommenden Stages verarbeitetet, änderte sich dann das Signal änderten sich auch die Werte in nachfolgenden Prozessen - auch wenn das nicht gewünscht wurde.
    Wir haben daran gelernt die Signale wie elektrische Leitungen zu betrachten, eben so wie sie nachher auch synthetisiert werden.
    Hier wird auch deutlich was damit gemeint ist wenn wir: "zu funktional gedacht haben".
    Die Lösung war die Signale zu takten indem die Inputs der Stages von der Clock abhängig gemacht gemacht wurden. Die so entstandenen Register implementierten  endlich das Pipelining implementiert.
    \item Alle Signale sollten nach Möglichkeit durch alle Stages geschleift werden, ob sie intern verarbeitetet würden oder nicht. Das führte zu zwei Problemen mit dem PC-Wert. Das erste Problem betraf die Initialisierung des Pc-Wertes, der mit jedem neuen Takt hochgezählt werden sollte um in der nächsten Fetch Vorgang den dem Korrekten Wert anzunehmen. Dieses Problem  haben wir zunächst durch entsprechendes Initial setzen passender Werte gelöst(4,3,2,1). Damit liefen bereits erste einfache Programme wie(?!?!?!?). Dieser Ansatz scheiterte aber sobald das Programm Sprünge verlangte. Zwar wurde die Sprungadresse richtig gesetzt, da es keinen Flush für die Pipeline gibt wurden die nun obsoleten weiter propagiert. %Welche obsoleten?
    Das Problem lösten wir indem wir den pc unabhängig von den Stages durch ein eigenes Registern speicherten und (was nochmal?)
    \item Ein ähnliches Problem ergab sich mit den Flags. Diese sollten auch durch die Stages geschleift werden. Da sie so aber immer auch als Input Wert propagiert wurden führt das ggf zu unerwünschten Verhalten. Wir hätten also einen Mechanismus einführen müssen um zu Testen ob der richtige Wert anliegt. Wir haben uns entschieden die Flags ebenfalls als unabhängige Signale zu setzten. Diese werden nun stetig berechnet und liegen als Werte an. Stages bzw. Operationen welche die die Flags benötigen greifen dann auf sie zu wenn sie diese benötigen(wie nochmal genau)
\end{itemize}

% Was will dieses Kapitel genau?
    %Vorerst Probleme sammeln die Bei der Arbeit aufgetreten sinnd
% Probleme konkret benennen?
% Inwieweit findet sich das unten wieder
%Eindampfen
% später drüber reden (luca und max?)

\section{Struktur oder Setup oder wie das aussieht}
Struktur und Kommunikation -
% Zusammenhänge von componenten
% Wie fließen daten durch den prozessor
% componenten, entities, architectures - vhdl setup
%Datentypen
%Packages und Components - Subtypen

%mpure muss in architectture kann aber auf werte außerhalb der funkttion zugreifen solnag sie in der architectue ssind

Die CPU ist in fünf \textit{Stages} unterteilt.
Befehle \enquote{fließen} durch diese Stages hindurch.
Jede dieser Stages besitzt Input- und Outputsignale.
Die Inputsignale einer jeden Stage werden \textit{clocked} beschrieben.
Sie werden somit als Register synthetisiert.
Der Prozess, der dies veranlasst, besitzt das Suffix \texttt{\_pipeline}.
Die Outputs einer jeden Stage sind nicht geclocked und somit nur \enquote{einfache} Datenleitungen.

\begin{description}
  \item[Fetch]
  Die Fetch Stage ist recht simpel.
  In ihr wird die Input Instruction clocked gelesen und auf den Output gelegt.
  % TODO really? it seems like, its outputs are never used.
  \item[Instruction Decode]
  Im Instruction Decode wird die gelesene Instruktion dem Decoder präsentiert und die Outputs des Decoders genutzt um entsprechende Register und Flags zu lesen.
  \item[Execute]
  Die Execute Stage legt entsprechende Signale an die ALU-Komponente an und gibt das Resultat aus.
  \item[Memory Access]
  Hier passieren zwei verschiedene Dinge.
  Zum Einen wird entschieden, ob gesprungen wird oder nicht.
  Dies wird als Outputsignal der nächsten Stage präsentiert.
  Zum Anderen wird das \texttt{data\_addr} Signal entsprechend geschaltet, sofern der Opcode ein \texttt{STR} oder \texttt{LDR} ist, und Daten entweder geschrieben oder gelesen werden.
  % Irgendwie komisch vllt zwei sätze
  \item[Write Back]
  Im Write Back passieren wiederum zwei Dinge parallel.
  Auf Basis des \texttt{will\_jump} Signals wird entweder der Program Counter (PC) um eins erhöht oder auf das Resultat der Memory Access Stage gesetzt.
  Sofern der bearbeitete Befehl in ein Register schreiben soll, wird dies ebenfalls getan.
\end{description}

\subsection{Setup}

Die CPU ist in drei VHDL Entities unterteilt: Die \texttt{cpu} selbst, die \texttt{alu} und der \texttt{decoder}.
Die gesamte Pipelinelogik ist in der \texttt{cpu} Entity untergebracht.
Sie besitzt nach außen nur einen \texttt{clk} Input sowie Adress- und Datenleitungen für Anbindung an die Speicher\footnotemark.
\footnotetext{Unser Design geht von zwei getrennten Speichern für Daten und Instruktionen aus.}
Die dazugehörige Architektur \texttt{cpu\_arc} nutzt die \texttt{decoder} und \texttt{alu} Komponenten intern.
Es wird außerdem ein \texttt{cpu\_pkg} bereitgestellt, dass die \texttt{cpu} Komponente und oft verwendete Typen enthält.

Die \texttt{alu} Entity ist recht simpel gehalten.
Sie nimmt eine \enquote{Rechenaufgabe} mit zwei Operanden entgegen und gibt das Ergebnis sowie eventuelle Flags nach außen.
Da die ALU keinen internen Zustand hat, besitzt sie auch kein \texttt{clk} Signal.
Neben der \texttt{alu} Entity selbst beinhaltet das \texttt{alu\_pkg} die speziellen ALU Opcodes und die \texttt{alu} Component.

Der Decoder ist ähnlich zur ALU gehalten.
Die \texttt{decoder} Entity erhält nur eine Instruktion als Input.
Sie ist ebenso nicht \texttt{clk}-abhängig.
Die Outputs des Decoders bestimmen, welche Operation die ALU ausführt und welche Register gelesen werden.
Im \texttt{decoder\_pkg} sind die \texttt{decoder} Component und alle Opcodes definiert.

% Component begriff erläutern? (Am besten vorher)

\section{Input / Output}

Die CPU benötigt natürlich noch Speicher für Daten und Instruktionen um ein Programm ausführen zu können.
% Colloquial? "naturlich noch" weg?
Dieser wird in der \texttt{processor} Entity mit der CPU verbunden.
Es wurde die \texttt{sram2} Entity, welche uns von Andreas Mäder\footnotemark zur Verfügung gestellt worden ist, verwendet.
\footnotetext{Guter Mann. Kudos dafür! *bussy* und Herzchen}
Im \texttt{processor} werden zwei Instanzen dieses Speichers erzeugt - eine für Daten, eine für Instruktionen.
Der Adress-Input dieser Speicher ist mit den jeweiligen Adress-Outputs der CPU verbunden.
Da die Speicher nicht mit vollen 32 bit Adressen operieren, werden die Output-Adressen der CPU \enquote{beschnitten}.
Es ist somit nicht möglich den vollen 32 bit Adressraum zu nutzen.
So wird aber nicht der gesamte 32 bit Raum beim simulieren alloziert.

Des weiteren stellt der \texttt{processor} ein \texttt{clk} Signal für Speicher und CPU bereit.

\section{Register}
% write im writeback
% wie macht man das in vhdl richtig
% clocked access


%Undefinierte Werte gleichzeitig lese Schreibzugriff auf Register, weil nicht geclockt

%geclockte werte zuweisen sind unsere pipeline register,

%alle schreiboperationen in einen geclockten prozess

%pc ist nicht in der Register Bank sondern eine eigens register damit dieser auch in anderen Prozessen benutze werden kann

Zunächst stellte uns unser VHDL-Verständnis, wie an anderen Stellen, vor Probleme.
Insbesondere Herausforderungen kamen beim Umgang mit dem Write Back auf. Die CPU war fehlerhaft implementiert. Es wurde gleichzeitig ein Lese und ein Schreibprozess durchgeführt, was in einem Absturtz des Programmes resultierte. Diese Lese und Schreibzugriffe auf die Register waren nicht geclockt. Unsere Lösung bestand darin, nur einen Prozess überhaupt schreiben zu lassen. Der Umgang mit dem PC war ebenfalls etwas umständlich.
Die Problematik bezieht sich darauf, dass sie schwierig in anderen Prozessen benutzt werden kann, wenn sie in der Registerbank steht.
Die herausgearbeitete Lösung, die Schlussendlich in der Implementation manifestiert wurde, basiert darauf, dass die PC-Werte nicht in der Registerbank steht, sondern ein eigenes Register haben.
Somit können diese auch in anderen Prozessen benutzt werden.


\section{PC Handling}

%undefined wegen Wert durchschleifen 
Der Programm Counter war zunächst fehlerhaft, da auf jeder Pipeline Ebene der Wert der vorangegangenen Ebene übernommen wurde. Das hat zur Rückgabe 
%Rückgabe richtiges Wort
von undefined Werten geführt. Als erste Lösung wurden die Pipeline-Ebenen iterativ abgearbeitet. 

Das hat wiederrum zu fehlern in den Sprüngen geführt, da keine Werte mehr zurückgeschrieben wurden. Der Fehler beim Flush sah folgendermaßen aus: 
%Kann ich nicht gut erklären eventuell zusammen 
% ungefähr so: allle werte sind initialisiert stage 1: 4 stage 2 : 3 usw, jede stage verringert den wert um 1 damit so die idee, wenn die line durchgelaufen ist der pc wert am ende den richtiogen wert hat um die nächste sequence von fetch, decode usw hat. Bei einem Sprung wurde nicht die richtige andresse zurück gegeben da die werte in der pipeline noch weiter fdurchllaufen ( etwa so: pc werte: 4, 3, 2 ,1 - die laufen halt durch, dann sprung neue adresse ist 124 oder so aber die werte aus der sequence laufen noch durch.
Schlussendlich gelößt wurde das Problem in dem wir den PC von der Pipeline getrennt haben. Es gibt nun ein zentrales PC - Register mit dem das write-back beschrieben wird. Der Prozess ist getaktet, hängt also von der Clock ab. Gibt es keinen JUMP- Befehl aus dem memeorry access (ist die entsprechende Flag nicht auf 1 gesetzt), beschreibt das PC Register das PC Signal um 1. Ansonsten wird das Ergebnis aus dem memory access benutzt. 
%eventuell besser formulieren 
%wir haben das jettzt ungefähr so gemacht. wir haben den pc von der pipeline "sequennce" getrennt. Esss gibt ein zentrales pc register wir di geclockt im writeback beschriben , das ist ein clock ssensitiver porzess der von der clock abhängt. pc register beschreibt das pc signal und erhöhht um 1 außer es gibt nen jump aus memory access (flaggg set to 1) dann wird das resultata aus mem acces stage benutzt.



\subsection{Sprünge}
Setzt die ALU das cmp-flagg auf 1 wird das als Start für die Berechnung eines Jump-befehls interpretiert. Das wird im memory access gesehen. 

Zunächst wird der JUMP-befehl gefetcht. Der Decoder ließt das zugehörige Register und die ALU rechnet dann mit den entsprechenden Werten. Das Ergebnis wird in den memory access geschrieben. Dort wird dann das (???welches Signal eig) - Signal bestimmt. Der richtige PC Wert wird anschleßend ins write-back geschreiben. 
%fühlt sich irgenwie auch nicht richtig eingängig an der text


%will jump berechnen : wir gucken in memm acc obs nen jump bzw branch uindem wir checkenn ob die alu das cmp flagg setz, falls ja dann wiukll jumpo signal 1 else 0

% 4 NOOPS; JMP Befehl fetchen, Decode stage ließt entsprechende Register, ALU rechnet mit ensprechenden werten, Das resultat kommt in die memory access stage. dort wird das will_jump signla bestimmt wie oben beschrieben. Dementsprechend wird der PC wert im write back gesetzt.
% das pc handling

\section{Decoder}

% needs work

Der Decoder ist als eine eigene Schnittstelle definiert. Seine Aufgabe es ist den 32-Bit-Code von dem Instructionmemory zu interpretieren, deren Information zu filtern und diese den entsprechenden Outputs zuzuweisen.
% bissle lang (zwei und;s)
 {\color{red}
Wie im Kapitel des Befehlssatzes beschrieben, werden die 32-Bit-Befehle im Decoder in die einzelnen Informationsbausteine zerlegt.

Bild der \enquote{dekonstruktion}
\\
 00000  0  00000  00000   0 00000 00000 00000
\\

% dekonstruiert? besseres Wort?
%Dabei geht der Decoder zuerst die möglichen Befehlssätze der ersten 5 Bits durch, prüft ob das Immediate-Bit gesetzt wurde und gibt dann die entsprechenden Werte in die jeweiligen Outputs weiter.
Dabei gleicht der Decoder den ermittelten "Opcode" mit denen des Befehlssatzes. Nachdem eine übereinstimmung gefunden wurde, wird das Immediate-Bit geprüft. In Abhängigkeit des "Opcodes" und des Immediate-bits werden dann die weiteren Informationen vom Decoder weitergeleitet. 
Dies sind:

\begin{itemize}
 \item Alu-Opcode

 Der Alu-Opcode ist ein 5-Bit großer Befehlssatz für die Alu. Er wurde eingeführt um gleiche Verhaltensweisen der Alu bei unterschiedlichen "Opcodes" zusammenzufassen.

\item Register

 Hierbei handelt es sich um Register die entweder gelesen oder beschrieben werden sollen. 

\item Immediatewert

 Der Immediatwert ist eine Direktzuweisung die eine Zahl als auch eine Speicheradresse sein kann

\item "Write-Enanble"

 Vergleichbar mit einer Flag, ist das "Write-Enable" eine 1-Bit großes Signal die ausschließlich dafür benötigt wird um zu bestimmen, ob im Speicher gelesen oder geschrieben werden soll.
\end{itemize}

 Diese Informationen werden immer an die Outputs des Decoders angelegt, sodass es den anderen Prozessorkomponenten überlassen ist,auf welche Inforamtion sie vom Decoder zugreifen}


{\color{green}- Kann zum Instruction-Set hoch - }

% Präsizer formulieren!

Dadurch weiß der Decoder welche weiteren Bits gelesen werden müssen um so diese Informationen an die entsprechenden Outputs weiterzugeben.
Aus diesem Grund können einige Outputs ignoriert werden, da diese nicht benötigt werden.
Daher werden sie nicht neu \enquote{gesetzt}.
% der decoder setzt nichts, der "outputtet" nur
Hier wurde auch ein eigener Alu-Op-code eingeführt, da die Alu bei einige Befehlen die gleiche Funktionsweise hat.
Durch den Alu-Op-code wird weiterer redundanter Code gespart und somit eine effizientere Zuweisung ermöglicht.

% KV-Diagramm part relevant - don't care's

\begin{figure}
    \centering
    \begin{lstlisting}
My gang,
( ͡°( ͡° ͜ʖ( ͡° ͜ʖ ͡°)ʖ ͡°) ͡°)

my dollars
[̲̅$̲̅(̲̅ ͡° ͜ʖ ͡°̲̅)̲̅$̲̅]

my Voldemort
( ͡° ͜V ͡°)
    \end{lstlisting}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

Durch diese ganzen Reduzierungen werden unnötige Zuweisungen vermieden und eine schnellere Verarbeitung gefördert.

{\color{green}- Kann zum Instruction-Set hoch - }

% als eigene entity definiert
% don't cares  instruction_set_real.txt
% wie in instruction_set_real.txt ist uns in vielen fällen der output gewisser signale egal.
% don't cares vereinfachen die decoder logik signifikant.
% es ist zb mpglich fix, den y part des befehls als reg sel 1 und z als reg sel 2 zu setzen, da diese, wenn nicht relevant auch nicht gelesen werden
% die einzigen signale, die einer logik bedürfen sind:
% alu_op\_sel
% reg\_target
% reg\_select\_3
% write\_en

%592-642

\section{ALU}



Die ALU bekommt ihren Opcode vom Decoder.
Damit ergeben sich die Input-Felder des ALU-Opcodes, der beiden Operanden und des Carry Bits.
Herzstück der Implementierung ist ein Switch-Konstrukt, welches - basierend auf dem ALU-Opcodes - die entsprechende logische/arithmetische Operation auswählt und mit den beiden Input Feldern \texttt{op\_1} und \texttt{op\_2} durchführt.
Compare Befehle \texttt{comp\_eq} und \texttt{comp\_gt} werden kontinuirlich abgehandelt. Diese haben (weiter unten behandelte) eigene Rückgabe Vektoren.
Die Werte werden bis zum Ende der Code Abarbeitung durchgereicht und gegebenenfalls modifiziert.
Das stellt uns vor die gleiche Problematik wie bei der Jum
p-Implementation. Die ALU wird in der CPU falsch verwendet.
%falsche verwendung der alu, in der cpu aber wie genau?
Die berechnung der insgesamt 14 Funktione Lediglich bei logischen shift, musste der Umweg über eine unsigned Variable aus einem right\_shift Aufruf genommen werden. 

%outputs
Die Ergebnisse der beiden Compare Methoden werden als eigene Ausgabe Bits behandelt und als std\_logic zurückgegeben (unabhängig vom ALU Opcode).


%flags, fehlt noch 

%resultat, fehlt noch 


% erst outputs beschreiben beschreiben, dann die flags und resultat(overflow, carry out, cmp) aus operanden, nicht in switch drinnen 
%switch ergebnis abhängig vom alu opcode
%signale statt verktorem 

%Im write-back wird abschließend die Flag Wert gesetzt, nachdem dieser abhängig vom opcode ermittelt wurde.




Zuvor war die gesamte Implementation als geclockter Prozess aufgegriffen worden. In der jetzt vorliegenden Implementation sind die Logiken einzeln rausgezogen. Welche Variante effizienter ist haben wir (da, wie Eingangs erwähnt, Effizienz niedriger priorisiert war) nicht getestet.

Da in jedem Fall alle Bits angeguckt werden müssen , könnte die (ausgelagerten) comp\_gt und comp\_eq zu einer eigenen ALU synthetisiert werden. Ist dies in einem geclockten Prozess, sind auch andere Varianten für die synthetisieren zu ermöglichen.
Weiteres Optimierungspotential kommt in Betrachtung der Möglichkeit auf, dass man mehrere Signale in den Prozess werfen möchte. Diese synthetisierte Variante könnte womöglich schneller sein, da sie Signale später zusammengeführt werden müssen.

\subsection{,,Fun with Flags" ein Kapitel darüber das wir flags auf unterschiedliche weisen implementiert haben und dessen Probleme}

schwierigkeiten bei der implementation der flags
-compare
-overflow immer noch nicht richtig implementiert

\chapter{Software}
% chapter introduction missing

\section{Anforderungsanalyse}
Zu Beginn des Softwareentwicklungsprojektes bestand die erste Herausforderung darin, eine auf das Problem zugeschnittene Anforderungsbeschreibung zu erstellen.
Die grundsätzlichen Anforderungen konnten dabei prinzipiell in zwei Kategorien eingeteilt werden.
%sehr softwaretechnik basiert , haben wir überhaupt anforderunganalyse gemacht?
% Sehr hochtrabend

\subsection{Funktionale Anforderungen}
% ganz viel Geschwafel hier

% warum keine subsubsections?

Die funktionalen Anforderungen beschreiben, über welche Funktionen die Software am Ende des Entwicklungsprozesses verfügen muss.
Um eine realistisch zu bewältigende Menge an Funktionen auswählen zu können, war eine sehr kritische Selektion notwendig. 
%zu undeutlich, was ist kritische selektion? absatz hat kein inhalt, einen absatz was muss die software leisten
So gab es z.B. zu Beginn die Idee eine diskrete Simulationsumgebung zu erstellen. Allerdings wurde diese Möglichkeit wieder verworfen, da unklar war, wie lange die Implementation dieses Features gedauert hätte und wie lange die Entwicklung des Compilers selbst dauert. 
%% Beispielsweise stand zu Beginn die Möglichkeit einer diskreten Simulationsumgebung im  Raum, welche dann aber schnell wieder verworfen wurde, da unklar war, wie hoch der konkrete Zeitaufwand dafür gewesen wäre mangels der Kenntnis über den Compiler selbst.
%satz ist zu lang
%geschwafelt
Wir entschieden uns also zunächst dafür, uns ausschließlich auf den Compiler (Assemblercode zu Binärcode) zu fokussieren.
Die dafür notwendigen funktionalen Anforderungen konnten relativ schnell ermittelt werden.

\subsubsection{Primäre Funktionen}
Zum einen bestand das primäre Ziel darin, eine beliebige Textdatei mit Assemblercode automatisch einlesen zu können, aus dem eine neue Textdatei - bestehend aus Binärcode - generiert werden sollte. Zum anderen musste es eine geeignete Möglichkeit geben syntaktische Fehler innerhalb des Assemblercodes zu erkennen und auszugeben.

\subsubsection{Sekundäre Funktionen}
Neben den primären Funktionen sollte es die Möglichkeit geben innerhalb der Software Makros zu erstellen, welche Operationen ermöglichen, die nicht im Instruction Set explizit definiert sind. Beispielsweise besitzt unser Instruction Set keinen Multiplikationsbefehl (\texttt{MUL}).
Dennoch kann der \texttt{MUL}-Befehl benutzt werden, da er intern durch die russische Bauernmultiplikation ersetzt wird.
Zusätzlich dazu war unklar, inwieweit das Hardwareentwicklungsteam auf Hazards im Rahmen des Pipelining reagieren könnte. Aus diesem Grund musste ein softwaretechnischer work-around gefunden werden. 
Zu Beginn wurde lediglich festgelegt, dass wir nach jedem Befehl eine notwendige Anzahl NOOP-Befehle einfügen.
Da dieses Prinzip allerdings völlig ungeeignet ist - wofür eine Pipeline benutzen, wenn sie obsolet gemacht wird? - entschieden wir uns auf bestimmte Abhängigkeiten von Beginn an mit ein zu beziehen.
Z.B. flushing der Pipeline vor einem Jump-Befehl oder das Erkennen der Abhängigkeiten von genutzten Registern.

%%Das Ziel bestand darin eine beliebige Textdatei mit Assemblercode automatisch einlesen zu können, aus dem eine neue Textdatei - bestehend aus Binärcode - generiert werden sollte.
% das ist die essenz reicht für den gesamtem Kram davor
%%Zusätzlich dazu musste es eine geeignete Möglichkeit geben syntaktische Fehler innerhalb des Assemblercodes zu erkennen und auszugeben.
% wasserfall
%%Zuzüglich dazu sollten zwei wichtige Dinge während der Übersetzung passieren.
% wasserfall
%%Zum einen musste es die Möglichkeit geben innerhalb der Software Makros zu erstellen, welche Operationen ermöglichen sollten, die nicht im Instruction Set explizit definiert waren.
%%Beispielsweise besitzt unser Instruction Set keinen Multiplikationsbefehl (\texttt{MUL}).
%%Dennoch kann der \texttt{MUL}-Befehl benutzt werden, da er intern durch die russische Bauernmultiplikation ersetzt wird.
%%Zum anderen war zu diesem Zeitpunkt unklar, inwieweit das Hardwareentwicklungsteam auf mögliche Komplikationen im Rahmen des Pipelining reagieren konnte.
%unklar
%hazard thema als beispiel aufgreifen
%%Daher musste eine softwaretechnische Lösung entwickelt werden.
% was ist das genau? welche lösung wofür
%%Zu Beginn wurde lediglich festgelegt, dass wir nach jedem Befehl eine notwendige Anzahl NOOP-Befehle einfügen.
%%Da dieses Prinzip allerdings völlig ungeeignet ist - wofür eine Pipeline benutzen, wenn sie obsolet gemacht wird? - entschieden wir uns auf bestimmte Abhängigkeiten von Beginn an mit ein zu beziehen.
%%Z.B. flushing der Pipeline vor einem Jump-Befehl oder das Erkennen der Abhängigkeiten von genutzten Registern.
% Das beispiel nicht in klammern

\subsection{Technische Anforderungen}
Nach Fertigstellung der funktionalen Anforderungsanalyse gingen wir dazu über, eine Liste an technischen Anforderungen des Systems zu entwerfen.

\subsubsection{Änderbarkeit}
Aufgrund des (vorläufigen) Mangels an Fachkenntnis war es von sehr großer Bedeutung das System so zu gestalten, dass Änderungen an der Struktur des Assemblercodes oder an den funktionalen Anforderungen ohne großen Aufwand gemacht werden konnten.
% wie wirkt sich diese änderbarkeit aus?

\subsubsection{Erweiterbarkeit}
Da zu Beginn des Entwicklungsprozesses nur schwer abzuschätzen war, wie lange wir für die Implementation von bestimmten Funktionen brauchen, war es notwendig das System so zu gestalten, dass eine einfache Erweiterbarkeit von Funktionalitäten gegeben ist.
% was meint das konkret? Wodurch wird erweiterbarkeit erreicht?

\subsubsection{Korrektheit}
In Folge der Tatsache, dass Binärcode nur äußerst schwer zu Debuggen ist, war eine Hauptanforderung ein hohes Maß an Korrektheit der Übersetzung.
%100przent korrektheit schwierige formulierung
Dieser Punkt stellte sich im Nachhinein als komplexer heraus als erwartet, da wir mangels eines Simulators auf die Fertigstellung des Hardwareentwicklungsteams warten mussten, um das Programm ausreichend testen zu können.
%compiler test wären auch ohne hw team gegangen?
% Konfiguration vs Compiler funktioniert

\subsubsection{Vernachlässigte Qualitätsmerkmale}
Standardmäßig gibt es im Laufe einer jeden Softwareentwicklung Entscheidungen zu signifikanten Qualitätsmerkmalen zu fällen.
So entschieden wir uns bewusst dafür, dass die Effizienz des Programms eine sehr untergeordnete Rolle spielen sollte, da es praktisch ausgeschlossen war, dass wir große Mengen an Quellcode in sehr kurzer Zeit übersetzen müssen. 
%ewtas kürzen
Des Weiteren spielte eine gute Benutzbarkeit/Bedienbarkeit eine geringe Rolle, da die potenzielle Nutzergruppe hochgradig spezialisiert ist. 
%%Des Weiteren spielte aufgrund der Spezialisierung des Fachgebietes in dem wir uns bewegen, eine gute Benutzbarkeit des Programms eine geringe Rolle, da jeder der mit dem Programm arbeitet genug Expertise besitzen sollte, um eine Konsole zu nutzen.
%umformulieren 

\section{ANTLR}
%https://www.antlr.org/
In der Vorbereitung für die Entwicklung des Compilers haben wir verschiedene Möglichkeiten abgewogen, wie die von uns gesteckten Ziele am besten zu erreichen sind.
Die simpelste Möglichkeit wäre es gewesen den Quellcode direkt mit einem großen Switch Statement zu parsen und die einzelnen Befehle dann zu übersetzen.
Da dies jedoch schnell relativ unübersichtlich zu werden schien und eine Kontradiktion zu den gesetzten funktionalen Anforderungen gewesen wäre, haben wir Ausschau nach möglichen Tools gehalten, mit denen wir unsere Ziele effizienter erreichen.
Dabei sind wir auf ANTLR gestoßen.
ANTLR (ANother Tool for Language Recognition) ist ein Parser Generator zum Lesen und Verarbeiten von strukturierten Textdateien auf der Basis von Grammatiken, der unter anderem verwendet werden kann, um eigene Programmiersprachen zu entwickeln.
% cite ANTLR
Um einen Compiler mit ANTLR zu erstellen, benötigt man zunächst eine Grammatik, in der die formalen Regeln der Sprache beschrieben werden.
Die Grammatik besteht aus Parser- und Lexer-Regeln.
Dabei stehen die Parserregeln für die Struktur der Sprache (Nonterminale) und die Lexerregeln für die tatsächlichen Zeichen/Wörter (Terminale), die die Nonterminale ersetzen können.
Die Grammatik kann mit Hilfe von ANTLR zu Java Klassen compiliert werden.
ANTLR liefert dabei einen Lexer und einen Parser mit deren Hilfe ein Parsetree generiert werden kann.
Dazu wird einfach eine Datei mit dem Quellcode eingelesen und mithilfe von Lexer und Parser verarbeitet.
Der resultierende Parsetree kann mit Hilfe von Visitors durchlaufen werden.
Diese leisten die eigentliche Übersetzungsarbeit.

\section{Compiler Implementation}

\subsection{Grammatik}
%redundant zjm Absatz davor kann eventuell weg 
Innerhalb einer Grammatik müssen wir grundsätzlich zwischen zwei unterschiedlichen Prinzipien unterschieden.
Zum einen gibt es die sogenannten Parser-Regeln und zum anderen die Lexer-Regeln (Tokenizer-Regeln).

\subsubsection{Lexer}
Der Lexer stellt die unterste Ebene einer Grammatik dar.
Jede Lexer-Regel definiert dabei genau einen Token.
Ziel des Lexers ist es nun für jede mögliche Eingabe zu erkennen, um welches Token es sich handelt.
Hierbei gilt zu beachten, dass ein Token nicht zwangsläufig exakt definiert werden muss, da die Nutzung von regulären Ausdrücken möglich ist.
%listng austauschen, kommaenatre weg, in eine fig. rein 
\begin{lstlisting}
MOV: 'MOV'; // Als MOV-Token wird jeder Ausdruck der Form "MOV" erkannt
BINARY: '0b' ([0-1])+;  // „0b1001…“ wird erkannt
\end{lstlisting}
Zu Beginn des Prozesses werden so zunächst sämtliche Eingaben in Tokens umgewandelt, welche dann von den Parser-Regeln in einen Syntax-Baum überführt werden.

\subsubsection{Parser}
Die Parser-Regeln definieren den Ableitungsbaum der formalen Grammatik.
Dabei entspricht jede Verzweigung einer Parser-Regel, während jeder Blattknoten einem Token entspricht.

\begin{figure}[h]
\centering
% \includegraphics{Figure 3.2.1}
\caption{Syntaxbaum des Befehls: \texttt{MOV, r10, 10;}}
\end{figure}

Eine Parser-Regel hat immer die Form
%listng austauschen, kommaenatre weg, in eine fig. rein 
\begin{lstlisting}
start: start command | command; //Startregel der Grammatik
\end{lstlisting}
wobei die Regel entweder direkt ein Token nutzt oder auf eine weitere Regel verweist.
%eventuell redundant zu eingehender erklärung
Anhand der Ableitungsregeln kann also eine Struktur erstellt werden, auf welche nun spezifisch reagiert werden kann.
So ist es innerhalb des Programmes nun möglich auf jeden Knoten des Baumes, sowie seine Eltern- und Kindknoten zuzugreifen.
% eventuell hochziehen zu dem Einleitungstext
%ein Teil von 3.2 zu 3.3 

\subsection{Makros}
Da unser Compiler mehrere Aufgaben erfüllen soll, die nur sequentiell erledigt werden können, wird dieser Ablauf insgesamt drei mal durchlaufen.
Bei jedem Durchlauf wird ein anderer Arbeitsschritt erledigt.
%eventuell die 3 punkte in anführungszeichne
Die drei Schritte sind das Ersetzen von Makros durch ihre jeweilige Implementation, das Einfügen von NOOPS zur Vermeidung von Hazards und die eigentliche Übersetzung.
Makros wurden von uns mit eigenen Lexer- und Parser-Rules innerhalb der Grammatik implementiert.
Dies ermöglicht es uns die so definierten Makros wie normale Befehle im Quellcode zu verwenden.
Der Compiler kümmert sich dann darum die Mnemoniks durch entsprechende Funktionen zu ersetzen.
% erläutern: mnemoniks (vllt in ner fussnote)
Zum aktuellen Zeitpunkt haben wir die russische Bauernmultiplikation für positive Zahlen (MUL) und ein simples binäres Invertieren (NOT) durch Makros verwirklicht.
Es lassen sich leicht zusätzliche Makros hinzufügen, indem diese in der Grammatik definiert  und entsprechende Implementation im Compiler verwirklicht werden.
Bei der Verwendung von Makros muss beachtet werden, dass diese nicht immer \enquote{in place} implementiert werden können.
% was meint in place? erläútern.
Es ist daher empfehlenswert, zu dokumentieren, welche Register von den jeweiligen Makros verwendet werden, um Inkonsistenzen zu vermeiden.
Das Vorgehen des Makro-Visitors besteht also darin den übergebenen Quellcode größtenteils unverändert zurück zu geben und nur die Stellen, an denen ein Makro entdeckt wird, zu ersetzen.

\subsection{Jumplabels und NOOPS}
Im zweiten Übersetzungsschritt werden die jump-Labels zu absoluten Zeilenangaben übersetzt und NOOPS eingefügt.
Die Sprungadressen werden ermittelt, indem der Compiler mitzählt, wie viele Befehle bereits gesehen wurden.
%"gesehen werden" gut formuliert
Da Sprünge im Instruktionsspeicher über einen Befehlsindex implementiert sind, reicht dieser aus um Sprünge mit Immediate zu realisieren.
Die Indizes werden zusammen mit den Labels gespeichert, sodass die Labels im Übersetzungsschritt durch die Immediate Werte ersetzt werden können.
Eine weitere Aufgabe innerhalb dieses Schrittes ist das Einfügen von NOOPS, um Hazards zu vermeiden.
Um dies zu bewerkstelligen, merkt sich der Compiler für jedes Register die Anzahl der notwendigen NOOPS, die eingefügt werde müssten, falls das Register im aktuellen Befehl vorkommt.
Ist der Wert 0 wird für das jeweilige Register der Wert auf 4 gesetzt, was der Anzahl der Pipelinestufen entspricht.
Falls der Wert nicht 0 ist, werden entsprechend viele NOOPS eingefügt.
%was heißt entsprechend viele ? 
%anmerken dass nach jedem Schritt dekrementiert wird
Außerdem werden generell nach jedem Sprung (\texttt{JMP}, \texttt{B}) 3 NOOPS eingefügt, da wir für den Prozessor keinen Flush implementiert haben.
% Warum nur 3 nicht 4??
Auf diese Weise wird vermieden, dass ungewollte Befehle in die Pipeline geladen werden.

\subsection{Übersetzung}
Im letzten Schritt wird der auf diese Weise modifizierte Quellcode in den Maschinencode übersetzt.
Die einzelnen Befehle werden je nach Befehlstyp (NOOP, TWOOP, THREEOP, Jump) unterschiedlich behandelt.
Die Opcodes stehen in einer Hashmap zusammen mit den Mnemoniks für die einzelnen Befehle.
Ist der letzte Operand ein Immediate Wert wird das Immediate-Bit wird entsprechend gesetzt.
Die Immediates werden aus dem angegebenen Zahlensystem (binär, hexadezimal, dezimal) zu einer Binärzahl übersetzt.
Alle Operanden werden je nach Befehlsstruktur mit einem entsprechendem Padding versehen.
%erklären was ein padding ist, mehr ausführen, bild bei befehlswörtern?
Anschließend wird das Befehlswort zusammengesetzt und zusammen mit einem führenden Befehlsindex zurückgegeben.
%was ist ein führender Befehlsindex

%\subsection{Probleme}
%Die Implementation des Compilers verlief relativ Problemlos und das Ergebnis ermöglicht uns alle von uns gesetzten Ziele zu erreichen. Eine Schwäche unseres Vorgehens war jedoch die schlechte Testbarkeit des Programms, da diese von der Implementation der Hardware abhängig war. Das führte dazu, dass Fehler erst spät sichtbar wurden, da wir den compilierten Quellcode erst an dem fertigen Prozessor testen konnten (siehe Kapitel Test).
%Simulator hätte das lösen können

\chapter{Ergebniss}

% Gleicher ablauf wie bei Vortrag
% Prozess von assembler zu maschinencode zu cpu, mit "fertigem" fibonnaci im ram
% Anwedung des beschrieben codes (hardware wie software) zeigen.
% -> Verweis auf assembler / machinencode


\begin{itemize}
    \item Wie lief das beim Fibonacci Test
    \item Padding passte nicht
    \item Nicht passende NOOPS
    \item manuelles Fixen des Mashinencodes
    \item Problem mit Carry-flags (Durchschleifproblem)
    \item Problem mit Jumpaddressen (Durchschleifproblem)
\end{itemize}

Unser next\_seq\_pc ansatz war nicht ganz durchdacht.
Die Idee war den nächsten PC in der Fetch stage schon zu bestimmen und \enquote{durchzuschleifen} bis zur Verwendung in memory access.
Dort wird dieser verwendet um die Instruction im fetch zu bestimmen.
Wenn nun in allen Pipelineregistern dieser next\_seq\_pc mit 0 initialisiert wird, fetchd der prozessor auch 4 mal die instruction an der Stelle 0.
Da die fetch stage auch auf basis dieses Wertes den nächsten next\_seq\_pc berechnet hat (in diesem fall 1) ist die instruction für die nächsten 4 instruction die an Adresse 1.

Dieses Problem zeigte sich auch bei den Flags und bei den Jumpaddressen.
Wenn ein Flag gesetzt werden sollte, setze die ALU dieses flag, wenn nicht wurde der vorherige Flagwert gesetzt.
Da das \enquote{setzen} in der ALU und das schreiben im Writeback einige Tacktzyclen ausseinander lag, lass der nachfolgende Befehl die \enquote{alten} Flags und setzte diese im Writeback direkt wieder.
Somit war es nicht möglich flags, wie zB. für compare / branch operation notwending zu setzen.
Jumps waren von diesem Durchschleifproblem ebenso betroffen. Ein neu gesetzer PC wurde von den nachfolgenden next\_seq\_pc Werten wieder überschrieben.

Um dies zu beseitigen änderten wir die Flag und PC logik komplett.
Das in der mem access stage verwendete Flag flags\_comp um ein branch auszuwerten wird im inst decode gelesen und bis zum mem access durchgeschliffen.
Es ist unabhängig von den von der alu gestetzten flags, die bis zum writeback durchgeschliffen wird.
Dadurch müssen zwar mehr NOOPS eingeführt werden zwischen compare und branch aber beide \enquote{Seiten} des flag setzens sind sauber getrennt.

Der PC wird nun \enquote{zentral} gehalten.
Es gibt ihn nurnoch einmal.
Die Writeback stage entscheidet nun mithilfe eines \enquote{will\_jump} signals, ob gesprungen wird und passt diesen bei Bedarf an.
Wenn nicht gesprungen wird, erhöht die write back stack den PC um 1.

Ein weiteres Issue, dass während der ersten Tests auftrat, war die Anzahl der NOOPS.
Es brauchte einiges herumprobieren um heraus zu finden, wie viele NOOPS wir bei jedem Befehl benötigen.

In unserem finalen Versuch war es uns möglich, ein Fibonacci-programm ablaufen zu lassen.

In früheren Überlegungen fiel uns auf, dass ein SUB nichts anderes ist als ein ADD mit einem Operant bitweise invertiert.
Deshalb war SUB als ADD aluop implementiert.
Wir vergaßen dabei einen Operanden zu invertieren.
Dies lies sich recht leicht korrigieren.
Die ALU kennt nun die Operation SUB.

\chapter{Konklusion und Ausblick}
\begin{itemize}
    \item HLT op
    \item Refactoring (unused signals)
    \item Overflow out
    \item link register is wrong
    \item Reset Signal / Initialization
    \item Pipeline flushing
\end{itemize}

% bewertung eigener Prozess
% Software: messen an gesetzten masstäben
% Reflektiv sein
% Bewertung des ergebniss
% Vergleich zu vorher / planungsphase

Es sind noch viele Möglichkeiten offen geblieben den Prozessor und den Assembler-Compiler auszubauen.
Zum Einen fehlen noch Implementationen für eine Opcodes.
Zum Anderen sind der Overflow der ALU und das Link Register nicht korrekt implementiert.

Um den Prozessor synthetisieren zu können müssen mindestens Signale richtig initialisiert werden.
Zur Zeit wird in VHDL ein Anfangswert gesetzt.
Hier wäre es nötig ein Reset-Signal einzuführen, dass den PC auf 0 und die Pipeline Register Opcodes auf NOOP setzt.
Auch Befehle wie COMPG nicht gut umgesetzt.
In VHDL lässt sich die dafür notwenidige Operation mit einem simplen \enquote{>} umsetzten.
Dies wird aber in der Synthese eine komplexe Schaltung erzeugen.

Der Prozessor im Zusammenspiel mit dem Assembler-Compiler kann aus seiner Pipelinestruktur noch nicht allzu viel herausholen.
Alle Abhängigkeite, die zu Hazards führen können, müssen eine entsprechende Anzahl von NOOPS nach sich ziehen.
Würde die CPU Pipeline Flushing implementieren ließe sich hier einiges an NOOPS sparen.
Man könnte noch einen Schritt weiter gehen und Instruction Reodering oder auch \enquote{Backpropagation} von Ergebnissen betreiben.

Eine HLT op wäre noch schön.

\kant[10-14]

\end{document}
